# 软件架构技术-大型网站技术架构与业务架构融合之道

## 一、操作系统

### 1）操作系统级别I/O

#### 概念介绍

* **应用程序内存：**通常代码用malloc/free、new/delete等分配出来的内存
* **用户缓冲区：**C语言的FILE结构体里面的buffer
* **内核缓冲区：**Linux的Page Cache。为了加快磁盘的I/O，将磁盘上的数据以Page为单位缓存在内存里

#### 缓冲I/O与直接I/O

* 缓冲I/O一次读/写操作有3次数据拷贝，磁盘→内核缓冲区→用户缓冲区→应用程序内存。
* 直接I/O一次读/写操作有2次数据拷贝，磁盘→内核缓冲区→应用程序内存。
* 即缓冲和直接I/O的区别在于前者有用户缓冲区，后者没有，因此后者的数据拷贝次数少一次。
* **注：无论是缓冲或直接I/O，应用程序写完后，数据是在内核缓冲区上的，如果不及时刷盘，系统断电后内存里的数据将消失。**

<img src="https://s3.bmp.ovh/imgs/2022/10/31/c150dfba72348178.png" alt="8YBzh.png"   width=600px/>
<br>



### 2）内存映射和零拷贝

* **内存映射文件：**相比较于直接I/O或缓冲I/O，内存映射文件使得用户空间不再使用物理内存，而是直接拿应用程序的逻辑内存地址映射到操作系统的内核缓冲区，读/写操作只有1次数据拷贝，磁盘→内核缓冲区。**Java的NIO可以使用MappedByteBuffer类实现内存映射文件，也可以使用直接I/O的写法。前者缺点是若文件太大将导致CPU占用高，后者缺点是慢。**

* **零拷贝：**对于从磁盘读取数据然后再发送到网络上的情况，可以在内核缓冲区和Socket缓冲区做映射的传递，而不需要进行内存上的数据拷贝。**Kafka消费消息时使用的零拷贝。Java的NIO中可以使用FileChannel.transferTo使用零拷贝。**

#### 直接I/O、内存映射文件、零拷贝对比

<img src="https://s3.bmp.ovh/imgs/2022/10/31/147034cc458a08cb.png"   width=450px/>

<img src="https://s3.bmp.ovh/imgs/2022/10/31/3dc286c8bb25bb41.png"   width=450px/>

<img src="https://s3.bmp.ovh/imgs/2022/10/31/2f9d4ec3f54ee655.png"   width=450px/>
<br>


### 3）网络I/O模型

* **四种IO模型：**同步阻塞I/O、同步非阻塞I/O、I/O多路复用、异步I/O

* **网络框架设计模式：**Reactor主动模式，应用程序不停轮询，询问操作系统或网络框架、I/O是否就绪，如select、poll、epoll、或NIO；Proactor被动模式，应用程序把读写操作全部交给操作系统或网络框架，由它们实际完成后再回调应用程序。


<img src = 'https://i.bmp.ovh/imgs/2022/06/09/f5e427baeaefaa96.png'  width=500/>
<br>

### 了解select、poll、epoll吗？

1. **select——>时间复杂度O(n)：**它仅仅知道了，有I/O事件发生了，却并不知道是哪那几个流，只能无差别轮询所有流，找出能读出数据，或者写入数据的流，对它们进行操作。所以select具有O(n)的无差别轮询复杂度，同时处理的流越多，无差别轮询时间就越长。
2. 时间复杂度O(n)：**poll本质上和select没有区别，它将用户传入的数组拷贝到内核空间，然后查询每个fd对应的设备状态， 但是它没有最大连接数的限制，原因是它是基于链表来存储的.
3. **epoll——>时间复杂度O(1)：**epoll可以理解为event poll，不同于忙轮询和无差别轮询，epoll会把哪个流发生了怎样的I/O事件进行通知。所以说epoll实际上是事件驱动（每个事件关联上fd）的，此时对这些流的操作都是有意义的。

| <span style="display:inline-block;width:70px">特点比较</span> | select | poll | epoll  |
| :----- | :-------: | :---------------: | :----------------------------: |
| 操作方式                                                     | 遍历                                                         | 遍历                                                       | 回调                                                         |
| 底层实现                                                     | 数组                                                         | 链表                                                       | 哈希表                                                       |
| 效率                                                       | 每次调用都进行线性遍历，O(n)                                 | 每次调用都进行线性遍历，O(n)                               | 事件通知方式，每当fd就绪，系统注册的回调函数就会被调用，将就绪fd放回到rdlist里面。O(1) |
| 最大连接数                                                   | 1024(32位)或2048(64位)                                       | 无上限                                                     | 无上限                                                       |
| fd拷贝                                                       | 每次调用select，都需要把fd集合从用户态拷贝到内核态           | 每次调用poll，都需要把fd集合从用户态拷贝到内核态           | 调用epoll_ctl时拷贝进内核并保存，之后每次epoll_wait不拷贝    |



#### epoll的原理和高效原因

* **epoll原理三步：**epoll_create生成监控和管理句柄fd的池子，epoll_ctl进行池子的增删改查（使用红黑树），epoll 池添加 fd 的时候顺便把fd 就绪之后的回调路径安排好。通过事件通知的形式，做到最高效的运行，epoll_wait让出CPU调度，有事就绪时立马通知。

* **select受到最大连接数限制原因：**select和poll的动作基本一致，只是poll采用链表来进行文件描述符的存储，而select采用fd标注位来存放，所以select会受到最大连接数的限制。
* **epoll高效原因：**
  * epoll内部管理 fd 使用了高效的红黑树结构管理，做到了增删改之后性能的优化和平衡，只需要生成一次池子，后续直接延用；select和poll每次开始前都要全量传入所有fd。
  * epoll池fd就绪后通过事件通知的形式，放置到epoll池的就绪列表ready_list中，可以直接返回给用户；select和poll则是轮询所有fd，对就绪fd进行标记，此后返回所有fd，让用户进行第二次轮询才知道哪些fd就绪。
  * select、poll都需要将有关文件描述符的数据结构拷贝进内核，最后再拷贝出来。而epoll创建的有关文件描述符的数据结构本身就存于内核态中，系统调用返回时利用 `mmap()` 文件映射内存加速与内核空间的消息传递，即 epoll 使用 mmap()  减少复制开销。
  * 虽然epoll的性能最好，但是在**连接数少并且连接都十分活跃**的情况下，select和poll的性能可能比epoll好，毕竟epoll的通知机制需要很多函数回调。

#### epoll的两种模式LT和ET

* **LT水平触发：**读缓冲区不为空就一直触发事件，写缓冲区不为满就一直触发事件。
* **ET边缘触发：**读缓冲区从空→满的时候触发一次，写缓冲区从满→空的时候触发一次。
* **缺点：**写缓冲区满的概率较小，当用户注册写事件但是没有数据要写时会一直触发，因此LT要避免写的死循环，写完数据后要取消写事件；若读缓冲区收到100字节数据，触发了一次读事件，而用户只读取了50个字节，剩下的50个字节将不会触发读事件，因此ET模式下触发读事件时必须一次性读完。
* **实际开发中一般使用LT水平触发，毕竟重复触发远好于漏事件，如Java中的NIO使用的是epoll的LT模式。**

<br><br><br><br><br><br><br><br><br><br><br>

#### 服务器编程的1+N+M模型

<img src="https://s3.bmp.ovh/imgs/2022/10/31/4138365d81922b0b.png" alt="1665738077453.png" title="1665738077453.png" />

<img src="https://s3.bmp.ovh/imgs/2022/10/31/f9ce6a05fc0c1d1c.png" alt="1665738158363.png" title="1665738158363.png" />

<img src="https://s3.bmp.ovh/imgs/2022/10/31/3b56de309c11539b.png" alt="1665738308209.png" title="1665738308209.png" />

<br>

### 4）进程、线程和协程

#### 为什么使用多线程？

* 因为服务器端的程序往往是I/O密集型的应用，**多线程可以提高CPU利用率和I/O吞吐**。

#### 为什么设计多进程？

* 多线程存在两个问题，即**线程内存共享要加锁，会导致并发效率下降；线程上下文切换存在一定开销**。
* 进程是资源分配的基本单位，进程间不共享资源，通过管道或Socket来通信。多进程对比多线程还减少线程在不同CPU核间切换的开销，且多进程相互独立。经典案例是Nginx，一个Master进程负责管理，N个Worker进程（内部单线程）相互独立，每个Worker进程对应一个CPU核，并行接收客户端请求。

#### 为什么设计多协程？

* **更好利用CPU：**线程调度由操作系统完成，应用程序无法干预，协程可由应用程序自己调度。
* **更好利用内存：**协程的堆栈大小不固定，用多少申请多少，内存利用率更高。

<img src="https://s3.bmp.ovh/imgs/2022/10/31/9dd5cd108f201bc5.png" alt="1665740368852.png" title="1665740368852.png" width="500"/>

<img src="https://s3.bmp.ovh/imgs/2022/10/31/5c7f501a3d4e3a1b.png" alt="1665740389139.png" title="1665740389139.png" width=500/>

<br>

### 5）内存屏障和CAS

#### 内存屏障

* 多读单写的情况下可以使用内存屏障，多线程写无法使用。内存屏障避免了指令的重排序。
* 在多核CPU里，每个CPU有自己的缓存，线程修改的值可能还在CPU的缓存中没有刷新到内存里，而内存屏障可以强制把这个值刷新到内存里面。

#### CAS

* 多线程写则可以用到CAS，基于CAS可以实现乐观锁、无锁队列、无锁栈、无锁链表等。



## 二、网络

### 1）HTTP 1.0的问题和解决方式

* **性能问题：**每个请求开一次TCP非常耗时。使用连接复用和Content-Length确定每个响应是否结束。
* **服务器推送问题：**服务端无法在客户端没有请求的情况下主动向客户推送消息。



### 2）HTTP 1.1的改进

* 默认使用连接复用，使用Chunk机制，响应的Body分成一块一块，用分隔符区分开，即使没有content-length也可以判断响应末尾。
* Pipeline机制，前一个请求没有收到响应即可发送下一个请求，缺点是队头阻塞，因此许多浏览器默认关闭Pipeline。
* **性能提升方式：**多个小图片组成一个大图一次请求（Spriting），JS拼接压缩，不同静态资源放在不同CDN上并发请求。
* **断点续传：**客户端记录已下载的数据量大小，在请求头部加Range：first offset - last offset确定下载范围，只适用于断点下载。



### 3）HTTP 2

* HTTP2不是用来替代HTTP1.1的，而是来解决性能问题，它兼容HTTP1.1。互联网中，HTTP1.1是必选的，HTTP2、SSL/TLS等都是可选项。
* **二进制分帧：**解决队头阻塞，每个请求和响应组成流，为每个流分配一个ID，把队头阻塞从HTTP请求粒度细化成帧粒度。
* 使用头部压缩减少请求报文长度。



### 4）SSL安全套接层和TLS传输层安全协议

#### 对称加密（无法保证密钥传输的安全性）

<img src="https://s1.ax1x.com/2022/10/27/xfxpff.png"   width=450px/>

#### 双向非对称加密（双方公钥如何安全传输）

* A向B发生信息时，先用自己私钥签名，然后用B的公钥加密。B收到后用私钥解密，然后用A的公钥验证签名。都是为了确保信息被篡改。

<img src="https://s3.bmp.ovh/imgs/2022/10/27/0f8ebe644fd7efb8.png"   width=450px/>

#### 单向非对称加密（公钥如何安全传输）

<img src="https://s3.bmp.ovh/imgs/2022/10/27/afe154a7115e8297.png"   width=450px/>

#### 中间人攻击（虚假公钥窃取信息）

<img src = 'https://s3.bmp.ovh/imgs/2022/10/27/0d2773ab30c890e0.png'   width=450px/>

#### 数字证书和证书认证中心CA

<img src = 'https://s3.bmp.ovh/imgs/2022/10/27/93ed3ebc0b68555d.png'   width=450px/>

#### SSL/TLS四次握手和HTTPS传输过程

<img src = 'https://s3.bmp.ovh/imgs/2022/10/27/c40f80cd8701c298.png'   width=300px/>

<img src = 'https://s3.bmp.ovh/imgs/2022/10/27/112c9b5be2f1a9a2.png'   width=350px/>



### 5）QUIC协议

* **基于UDP协议的多路并发传输协议（使用TCP必定会有队头阻塞问题），QUIC实现了SSL/TLS的全部功能和HTTP2的部分功能**。

* **UDP协议的丢包问题解决：**QUIC使用RAID5，每发送10个数据包就构建一个冗余包，允许10个丢失一个，超过这个数就重传。
  * **RAID5：**每发送5个数据包就发送一个冗余包，冗余包由5个数据包做异或运算得到，丢失一个可以通过其他几个包计算出来。
  * **RAID6：**每发送5个数据包就发送两个冗余包，其中一个由异或得到，另一个由另一种公式得到，允许丢失两个包，可以通过解二元一次方程重新算出。

* **更少RTT：**HTTPS需要7次握手，共3个RTT。QUIC可以把七次握手减为0次。
* **连接迁移：**TCP使用A-IP、A-PORT、B-IP、B-PORT四元组标识一组链接，当用户IP改变时就需要重新链接，而QUIC使用64位数字标识链接，即使IP改变也不会断链。



## 三、高并发问题

### 1）问题分类

#### 侧重于”高并发读“的系统

* **数量级差距：**读端可能是亿级，而写端可能只在百万或千万级别。
* **响应时间：**读端要求响应毫秒级，写端则可能几分钟或更长时间。
* **频率：**读的频率远高于写的频率
* **常见场景：**搜索引擎、电商商品搜索、电商系统商品描述

#### 侧重于“高并发写”的系统

* 如广告扣费系统，C端用户的广告浏览或点击都要尽快扣费广告主，要求及时响应

#### 同时侧重“高并发读写”的系统

* 无论读写端都面临高并发压力，用户规模在亿级别，要求读写的处理响应都及时。
* **常见场景：**电商的库存系统和秒杀系统、支付系统和微信红包、即时通信系统如QQ微信、微博朋友圈等。



### 2）高并发读处理策略

#### 策略1 加缓存（空间换时间）

* **案例一：本地缓存或Memcached/Redis集中式缓存。**缓存的更新分为两种，主动更新即数据库数据发生改变时主动删除或更新缓存中数据，被动更新即查询请求到来时发现缓存过期再更新缓存。缓存常见问题有缓存击穿、缓存穿透、缓存雪崩等。
* **案例二：MySQL的master/slave。**从库分担主库的读压力，但是需要考虑同步方式和一致性问题。
* **案例三：CDN静态文件加速。**静态文件缓存到各个节点，用户去不同CDN获取数据，由CDN自身负责从源系统获取资源缓存到本身节点。

#### 策略2 并发读

* **案例一：异步RPC。**要求RPC调用之前没有耦合关系，可进行异步调用。
* **案例二：Google的冗余请求。**假设有一个请求要100台服务器同时联合处理，每台服务器有1%的概率发生调用延迟（响应时间大于1s），则响应时间大于1s的概率高达1-99% ^ 100 = 63%。可以改为客户端首先给服务端发送一个请求，若在一定时间内（95%的响应时间）没有收到服务端响应，则马上给另一台或多台服务器发送同样请求，若后续收到第一个请求的响应则中止其他请求的处理。采用这种方法，可以仅用2%的额外请求将系统99.9%的请求响应时间从1800ms降到74ms。

#### 策略3 重写轻读

* **案例一：微博feeds流。**如微博或微信，用户关注多个人，系统需要将多个人的微博按时间排序成列表，也就是Feeds流展示给用户。
  * **初始方案：**查看自己发布的微博只需要一条SQL即可，但是查看关注的人的微博需要先查询关注列表再进行查询，无法满足高并发查询请求。
  * **修改方案——重写轻读：**不是在查询的时候进行聚合，而是事先聚合好，为每个user_id准备一个Feeds流或收件箱。每个用户有一个发件箱和收件箱，发布一条微博后进入发件箱，后台异步推送给粉丝的收件箱，用户读取时直接读取收件箱。将读的逻辑转成写的逻辑。
  * **实现细节：**收件箱列表在内存中，如Redis，由于会无限增长，限制数量如2000；用户的历史微博存在MySQL中，按照用户ID和月份分库；用户粉丝多时如果推送所有粉丝则计算量巨大，因此”推拉结合“，对于粉丝多的推送给在线用户，粉丝少的直接推送所有粉丝。
* **案例二：多表联合查询，宽表和搜索引擎。**简单的join可以加从库解决，但是若数据分库后，无法使用原生join，只能先分别取出来再聚合。此时若数据量大，分页排序计算量较高，无法在内存中进行。**可以采用重写轻读，提前计算好关联数据；或者准备宽表，把关联数据计算好后保存在宽表中，依据实际情况可以定时算或当原始表改变时自动触发；也可以使用ES搜索引擎，把多张表join结果做成文档，放在搜索引擎中，进行灵活的分页和排序查询功能。**

#### 策略总结——读写分离CQRS架构



### 3）高并发写处理策略

#### 策略1 数据分片

* **案例一：数据库的分库分表。**分表后即使在一个机器上，但可以更充分使用CPU、内存。分库后可以使用多台机器资源。
* **案例二：JDK的ConcurrentHashMap。**ConcurrentHashMap分槽实现高并发。
* **案例三：Kafka的Partition。**Kafka的topic是逻辑概念，实际上分为多个partition，对应多个日志文件，partition之间相互独立，可以并发读写。
* **案例四：ES的分布式索引。**大量数据索引只有一个的话会很长，为了并发查询，建立成多个小索引，并行查询后再进行合并。

#### 策略2 任务分片

* **案例一：CPU的指令流水线。**指令分阶段并发执行。
* **案例二：Map/Reduce。**任务划分后再合并结果。
* **案例三：tomcat的1+n+m网络模型。**把请求处理分工序，监听（1）、I/O（N）、业务逻辑处理（M）。

#### 策略3 异步化

* **案例一：短信验证码注册或登录。**服务器收到请求后放入消息队列，让后台任务去读取消息，调用第三方短信平台发送验证码。服务器与消息队列是内网通信，即使客户端并发量大也只会堆积再消息队列中，不会阻塞服务器。
* **案例二：电商订单系统。**创建订单后写入订单系统数据库，支付完成后服务器立即返回。实际后台拆分订单等不阻碍主流程的业务逻辑进行异步化处理。
* **案例三：广告计费系统。**用户的点击请求会以日志的形式落盘，然后服务器立即返回。后续的计算判断扣费等逻辑进行异步化处理。
* **案例四：LSM树（写内存+Redo log日志）。**支持KV存储，插入时K无序，但是落盘时按照K大小排序，即磁盘上要实现sorted HashMap。但并非插入同时就对磁盘进行排序。由于磁盘写入速度慢，因此直接在内存维护Sorted HashMap。为了避免系统宕机丢失数据，加入日志顺序写入，即Redo Log。再加上后台任务定期将内存的Sorted HashMap合并到磁盘中，执行磁盘数据的合并排序。
* **案例五：Kafka的pipeline。**leader不会主动给Follwer同步数据，而是等他们主动拉取。Leader与Follwer形成一个管道，消息是一批批流过管道的。Leader将任务分离，一个是接受和存储客户端消息，一个是同步消息到Follwer。

#### 策略4 批量

* **案例一：Kafka的百万QPS写入。**Kafka写入快的三个策略，一个是partition分片，一个是磁盘顺序写入（非随机写入），另一个是批量。Kafka客户端在内存中为每个partition准备一个内存队列RecordAccumulator，Producer线程发送的消息存入该内存队列，再通过Sender线程从队列中批量取出并发送。若同步发送则消息存入队列后Producer会阻塞，无法批量。
* **案例二：广告计费系统的合并扣费。**扣费模块一次性从持久化消息队列中取多条消息，同一组广告的消息扣费金额累加合并后一次性扣除。
* **案例三：MySQL的小事务合并机制。**互不关联的小事务合并起来变成一个事务，一次性执行。

#### 策略5 串行化+多进程单线程+异步I/O

* 例如Nginx和Redis，都是单线程模型，没有锁竞争和线程切换开销。且使用异步I/O可以将请求串行化。



### 4）容量规划

#### 概念

* 高并发读写的应对策略是定性分析，而压力测试和容量规划则是定量分析，分析需要部署多少台机器，准备多少资源。
* **吞吐量：**单位时间处理的请求数，QPS，TPS。**响应时间：**处理每个请求所需时间。**并发数：**并行处理请求个数。吞吐量✖响应时间=并发数。

<img src="https://s3.bmp.ovh/imgs/2022/10/31/c4b7bd83a603c088.png"   width=450px/>

#### 压力测试与容量评估

* 机器数=预估总流量/单机容量。预估总流量为服务峰值✖余量系数（如2倍或3倍），单机容量通过压力测试获得。预估总流量很容易导致大部分时间机器是闲置的，因此可以动态增减机器数量。
* **线上压力测试vs测试环境压力测试：**测试环境搭建麻烦且需要时刻跟随功能更新迭代，因此难以持续，更多采用线上压力测试。
* **读接口压力测试vs写接口压力测试：**读接口可以对线上流量进行重放，但是写接口这样做会带来很多测试数据。因此，一种方式是摘取流量，不重放流量，只是把线上的真实流量划一部分集中导入集群中的几台机器中；另一种方式是部署影子数据库，对测试数据打标签进入影子数据库，通过数据库中间件即可实现。





# Java中内存映射、零拷贝、非阻塞IO与多路同步复用IO、异步IO代码实例。

```java
// 内存映射
public static void main(String[] args) throws IOException {
	FileChannel channel = FileChannel.open(Paths.get("test.txt"),
            StandardOpenOption.READ, StandardOpenOption.WRITE);
    MappedByteBuffer mappedByteBuffer = channel.map(FileChannel.MapMode.READ_WRITE, 0, channel.size());
    byte[] bytes = new byte[1024];
    mappedByteBuffer.get(bytes);
    mappedByteBuffer.put(bytes);
    mappedByteBuffer.position(0);
    channel.close();
}
```

```java
// 零拷贝
public static void main(String[] args) throws IOException {
	FileChannel inputChannel = FileChannel.open(Paths.get("input.txt"),
            StandardOpenOption.READ, StandardOpenOption.WRITE);
	FileChannel outputChannel = FileChannel.open(Paths.get("output.txt"),
            StandardOpenOption.READ, StandardOpenOption.WRITE);
    long size = inputChannel.size();
    long position = inputChannel.position();
    inputChannel.transferTo(position, size, outputChannel);
    inputChannel.close();
    outputChannel.close();
}
```

```java
// 多路复用IO 即非阻塞IO
public class NIOServer {
    private static final Logger LOGGER = LoggerFactory.getLogger(NIOServer.class);

    public static void main(String[] args) throws IOException {
        Selector selector = Selector.open();
        ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();
        serverSocketChannel.configureBlocking(false);
        serverSocketChannel.bind(new InetSocketAddress(1234));
        serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT);

        while (true) {
            if(selector.selectNow() < 0) {
                continue;
            }
            //获取注册的channel
            Set<SelectionKey> keys = selector.selectedKeys();
            //遍历所有的key
            Iterator<SelectionKey> iterator = keys.iterator();
            while(iterator.hasNext()) {
                SelectionKey key = iterator.next();
                iterator.remove();
                //如果通道上有事件发生
                if (key.isAcceptable()) {
                    //获取该通道
                    ServerSocketChannel acceptServerSocketChannel = (ServerSocketChannel) key.channel();
                    SocketChannel socketChannel = acceptServerSocketChannel.accept();
                    socketChannel.configureBlocking(false);
                    LOGGER.info("Accept request from {}", socketChannel.getRemoteAddress());
                    //同时将SelectionKey标记为可读，以便读取。
                    SelectionKey readKey = socketChannel.register(selector, SelectionKey.OP_READ);
                    //利用SelectionKey的attache功能绑定Acceptor 如果有事情，触发Acceptor
                    //Processor对象为自定义处理请求的类
                    readKey.attach(new Processor());
                } else if (key.isReadable()) {
                    Processor processor = (Processor) key.attachment();
                    processor.process(key);
                }
            }
        }
    }
}
```

```java
// 异步IO,Future & CompletionHandler
public static void main(String[] args) throws IOException {
	AsynchronousFileChannel channel = AsynchronousFileChannel.open(Paths.get("input.txt"));
    ByteBuffer buffer = ByteBuffer.allocateDirect(1024 * 1024 * 100);
	Future<Integer> result = channel.read(buffer, 0);        
	while(!result.isDone()) {
		System.out.println("reading file, I can do other work.");
	}
    channel.close();
}

public static void main(String[] args) throws IOException {
	byte[] data = { 2, 3, 5, 7, 11, 13, 17, 19, 23 };
	ByteBuffer buffer = ByteBuffer.wrap(data);
    CompletionHandler<Integer, Object> handler = new CompletionHandler<Integer, Object>() {
		@Override
		public void completed(Integer result, Object attachment) { // success
			System.out.println("Bytes written: " + result);
		}
		@Override
		public void failed(Throwable exc, Object attachment) { // failed
			System.out.println("Asynch write failed: " + exc.getMessage());
		}
	};
    AsynchronousFileChannel channel = AsynchronousFileChannel.open(Paths.get("primes.txt"),
                StandardOpenOption.CREATE, StandardOpenOption.WRITE);
    channel.write(buffer, 0, null, handler);
    channel.close();
}
```





